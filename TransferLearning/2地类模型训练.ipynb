{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "import numpy as np;\n",
    "import os.path;\n",
    "from tensorflow.contrib.tensorboard.plugins import projector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分类数\n",
    "class_num=21;\n",
    "#运行次数\n",
    "max_steps=1001;\n",
    "#图片数量\n",
    "batch_size=50;\n",
    "#文件路径\n",
    "image_path=\"F:/下载资源/地类数据/UCMerced_LandUse/Images/\";\n",
    "#训练率\n",
    "lr=0.001;\n",
    "#模型保存路径\n",
    "save_path=\"F:/下载资源/地类数据/UCMerced_LandUse/model/\";\n",
    "#tensorboard保存路径\n",
    "board_path=\"F:/下载资源/地类数据/UCMerced_LandUse/log/\";\n",
    "\n",
    "#获取训练文件下的所有数据\n",
    "#返回图片路径列表和标签列表\n",
    "def get_AllImg(path):\n",
    "    img_list=[];\n",
    "    lable_list=[];\n",
    "    for sub_dir in os.listdir(path):\n",
    "        for img_name in os.listdir(path+sub_dir):\n",
    "            lable_list.append(sub_dir);\n",
    "            img_list.append(path+sub_dir+\"/\"+img_name);\n",
    "    #创建临时数据数组\n",
    "    temp=np.array([img_list,lable_list]);\n",
    "    #数组转置\n",
    "    temp=temp.transpose();\n",
    "    #随机搅乱数据\n",
    "    np.random.shuffle(temp);\n",
    "    #重新获取数据\n",
    "    img_list1=list(temp[:,0]);\n",
    "    lable_list1=list(temp[:,1]);\n",
    "    return img_list1,lable_list1;\n",
    "\n",
    "#产生训练批次数据\n",
    "def get_batchs(imgs,labs,resize_w,resize_h,batch_size,capacity):\n",
    "    #转换tensorflow数据类型\n",
    "    img=tf.cast(imgs,tf.string);\n",
    "    lab=tf.cast(labs,dtype=tf.int64);\n",
    "    #实现一个输入的队列。\n",
    "    queue=tf.train.slice_input_producer([img,lab]);\n",
    "    lable=queue[1];\n",
    "    image=tf.read_file(queue[0]);\n",
    "    image_c=tf.image.decode_jpeg(image,channels=3);\n",
    "    #图片尺寸转换\n",
    "    image_c=tf.image.resize_image_with_crop_or_pad(image_c,resize_w,resize_h);\n",
    "    # (x - mean) / adjusted_stddev  调整后的stddev\n",
    "    image_c = tf.image.per_image_standardization(image_c);\n",
    "    #生成批次对象\n",
    "    image_batch,lable_batch=tf.train.batch([image_c,lable],batch_size=batch_size,capacity=capacity);\n",
    "    # 转化图片\n",
    "    image_batch=tf.cast(image_batch,tf.float32);\n",
    "    #重新定义下 label_batch 的形状\n",
    "    lable_batch=tf.reshape(lable_batch,[batch_size]);\n",
    "    return image_batch,lable_batch;\n",
    "\n",
    "#初始化权重\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape,stddev = 0.01))\n",
    "#init weights权重\n",
    "weights = {\n",
    "    #卷积盒的为 3*3 的卷积盒，图片厚度是3，输出是16个featuremap\n",
    "    \"w1\":init_weights([3,3,3,16]),\n",
    "    \"w2\":init_weights([3,3,16,128]),\n",
    "    \"w3\":init_weights([3,3,128,256]),\n",
    "    \"w4\":init_weights([4096,4096]),\n",
    "    \"wo\":init_weights([4096,class_num])\n",
    "    }\n",
    "\n",
    "#init biases偏移量\n",
    "biases = {\n",
    "    \"b1\":init_weights([16]),\n",
    "    \"b2\":init_weights([128]),\n",
    "    \"b3\":init_weights([256]),\n",
    "    \"b4\":init_weights([4096]),\n",
    "    \"bo\":init_weights([class_num])\n",
    "    }\n",
    "\n",
    "#定义卷积池化操作\n",
    "#采用relu激活函数\n",
    "def conv2d(x,w,b):\n",
    "    x=tf.nn.conv2d(x,w,strides=[1,1,1,1],padding=\"SAME\");\n",
    "    x=tf.nn.bias_add(x,b);\n",
    "    return tf.nn.relu(x);\n",
    "\n",
    "def pooling(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\");\n",
    "\n",
    "#不知道这个是什么操作\n",
    "#归一化操作？\n",
    "def norm(x,lsize=4):\n",
    "    return tf.nn.lrn(x,depth_radius=lsize,bias=1,alpha=0.001/9.0,beta=0.75);\n",
    "\n",
    "#定义训练模型\n",
    "def model(images):\n",
    "    l1 = conv2d(images,weights[\"w1\"],biases[\"b1\"])\n",
    "    l2 = pooling(l1)\n",
    "    l2 = norm(l2)\n",
    "    l3 = conv2d(l2,weights[\"w2\"],biases[\"b2\"])\n",
    "    l4 = pooling(l3)\n",
    "    l4 = norm(l4)\n",
    "    l5 = conv2d(l4,weights[\"w3\"],biases[\"b3\"])\n",
    "    #same as the batch size  与批次大小相同\n",
    "    l6 = pooling(l5)\n",
    "    l6 = tf.reshape(l6,[-1,weights[\"w4\"].get_shape().as_list()[0]])\n",
    "    l7 = tf.nn.relu(tf.matmul(l6,weights[\"w4\"])+biases[\"b4\"])\n",
    "    soft_max = tf.add(tf.matmul(l7,weights[\"wo\"]),biases[\"bo\"])\n",
    "    return soft_max\n",
    "\n",
    "def model1(images, batch_size, n_classes):\n",
    "\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "     # 卷积盒的为 3*3 的卷积盒，图片厚度是3，输出是16个featuremap\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[3, 3, 3, 16],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[16],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(images, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('pooling1_lrn') as scope:\n",
    "            pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pooling1')\n",
    "            norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "                weights = tf.get_variable('weights',\n",
    "                                          shape=[3, 3, 16, 16],\n",
    "                                          dtype=tf.float32,\n",
    "                                          initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))\n",
    "                biases = tf.get_variable('biases',\n",
    "                                         shape=[16],\n",
    "                                         dtype=tf.float32,\n",
    "                                         initializer=tf.constant_initializer(0.1))\n",
    "                conv = tf.nn.conv2d(norm1, weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "                pre_activation = tf.nn.bias_add(conv, biases)\n",
    "                conv2 = tf.nn.relu(pre_activation, name='conv2')\n",
    "\n",
    "    # pool2 and norm2\n",
    "    with tf.variable_scope('pooling2_lrn') as scope:\n",
    "        norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "        pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='SAME', name='pooling2')\n",
    "\n",
    "    with tf.variable_scope('local3') as scope:\n",
    "        reshape = tf.reshape(pool2, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[dim, 128],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[128],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "\n",
    "    # local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[128, 128],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[128],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name='local4')\n",
    "\n",
    "    # softmax\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = tf.get_variable('softmax_linear',\n",
    "                                  shape=[128, n_classes],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[n_classes],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        softmax_linear = tf.add(tf.matmul(local4, weights), biases, name='softmax_linear')\n",
    "\n",
    "    return softmax_linear\n",
    "\n",
    "#定义评估\n",
    "def loss(logits,lable_bathes):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits \\\n",
    "            (logits=logits, labels=lable_bathes, name='xentropy_per_example');\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name + '/loss', loss)\n",
    "    return loss\n",
    "\n",
    "#定义损失函数\n",
    "def accuracy(logits,lable):\n",
    "    acc=tf.nn.in_top_k(logits,lable,1);\n",
    "    acc=tf.cast(acc,tf.float32);\n",
    "    return acc;\n",
    "\n",
    "#定义训练方式\n",
    "def train(loss,lr):\n",
    "    train_op=tf.train.RMSPropOptimizer(lr,0.9).minimize(loss);\n",
    "    return train_op;\n",
    "\n",
    "#训练模型\n",
    "def run_training():\n",
    "    #1、获取所有数据\n",
    "    image,lable=get_AllImg(image_path);\n",
    "    print(\"数据获取完成\");\n",
    "    #2、获取批次数据\n",
    "    image_batches, label_batches =get_batchs(image,lable,256,256,batch_size,20);\n",
    "    #3、cnn模型计算\n",
    "    train_logits =model1(image_batches,batch_size,class_num);\n",
    "    #4、模型评估\n",
    "    cost=loss(train_logits,label_batches);\n",
    "    #5、模型训练\n",
    "    train_op=train(cost,lr);\n",
    "    #6、验证\n",
    "    acc=accuracy(train_logits,label_batches);\n",
    "\n",
    "    #创建sess\n",
    "    sess=tf.Session();\n",
    "    #初始化变量\n",
    "    init=tf.global_variables_initializer();\n",
    "    sess.run(init);\n",
    "\n",
    "\n",
    "    try:\n",
    "        for step in np.arange(max_steps):\n",
    "            _, train_acc, train_loss = sess.run([train_op, acc, cost])\n",
    "            #每100步打印一次\n",
    "            if step % 100 == 0:\n",
    "                print(\"loss:{} accuracy:{}\".format(train_loss, train_acc))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"Done!!!\")\n",
    "    finally:\n",
    "        sess.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_training();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
